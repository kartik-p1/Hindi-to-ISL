{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb02d10e-b557-445b-9992-26621bcc0e06",
   "metadata": {},
   "source": [
    "# CS 689A Project - Hindi To Indian Sign Language: Rule-Based Translation System\n",
    "### Done by\n",
    "1. Yashvir Singh Nathawat(231110059)\n",
    "2. Karthik Jain (231110023)\n",
    "3. Aditya Katare (231110005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "838562f8-d416-4c8d-9988-0da61144732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import stanza\n",
    "import nlp\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3fcb2-4549-4e7d-ba44-5e4683fec551",
   "metadata": {},
   "source": [
    "# Input the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "50408616-55ff-4321-803c-03e5ec85dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the hindi sentence :  मैं खाने नहीं जाऊँगा।\n"
     ]
    }
   ],
   "source": [
    "sentence = input('Enter the hindi sentence : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051582-db9b-4947-81c1-7623f666f2cb",
   "metadata": {},
   "source": [
    "# POS Tagging and Dependency Parsing - STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aea6b3b4-3730-4745-9f15-a0d62dca1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 12:02:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "2024-04-25:12:02:47,156 INFO     [core.py:214] Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba8684bd4204f21b74fdc7a49002d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 12:02:47 INFO: Downloaded file to C:\\Users\\Lenovo\\stanza_resources\\resources.json\n",
      "2024-04-25:12:02:47,740 INFO     [common.py:156] Downloaded file to C:\\Users\\Lenovo\\stanza_resources\\resources.json\n",
      "2024-04-25 12:02:48 INFO: Loading these models for language: hi (Hindi):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | hdtb          |\n",
      "| pos       | hdtb_charlm   |\n",
      "| lemma     | hdtb_nocharlm |\n",
      "| depparse  | hdtb_charlm   |\n",
      "=============================\n",
      "\n",
      "2024-04-25:12:02:48,282 INFO     [core.py:268] Loading these models for language: hi (Hindi):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | hdtb          |\n",
      "| pos       | hdtb_charlm   |\n",
      "| lemma     | hdtb_nocharlm |\n",
      "| depparse  | hdtb_charlm   |\n",
      "=============================\n",
      "\n",
      "2024-04-25 12:02:48 INFO: Using device: cpu\n",
      "2024-04-25:12:02:48,283 INFO     [core.py:287] Using device: cpu\n",
      "2024-04-25 12:02:48 INFO: Loading: tokenize\n",
      "2024-04-25:12:02:48,284 INFO     [core.py:293] Loading: tokenize\n",
      "2024-04-25 12:02:48 INFO: Loading: pos\n",
      "2024-04-25:12:02:48,290 INFO     [core.py:293] Loading: pos\n",
      "2024-04-25 12:02:48 INFO: Loading: lemma\n",
      "2024-04-25:12:02:48,555 INFO     [core.py:293] Loading: lemma\n",
      "2024-04-25 12:02:48 INFO: Loading: depparse\n",
      "2024-04-25:12:02:48,583 INFO     [core.py:293] Loading: depparse\n",
      "2024-04-25 12:02:48 INFO: Done loading processors!\n",
      "2024-04-25:12:02:48,814 INFO     [core.py:345] Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PSP', 'NN', 'VM', 'NNP', 'VAUX', 'SYM', 'JJ', 'NNPC', 'PRP', 'CC', 'NNC', 'QC', 'NST', 'DEM', 'RP', 'QF', 'NEG', 'RB', 'QCC', 'QO', 'INTF', 'JJC', 'WQ', 'RDP', 'UNK', 'PRPC', 'NSTC', 'RBC', 'QFC', 'CCC', 'INJ']\n"
     ]
    }
   ],
   "source": [
    "# Import Stanza Hindi Pipeline\n",
    "nlp = stanza.Pipeline('hi', processors='tokenize,lemma,pos,depparse')\n",
    "# Known POS Tags\n",
    "print(nlp.processors['pos'].get_known_xpos())\n",
    "doc = nlp(sentence)\n",
    "#print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "33f6665f-0f38-41bf-9027-df63b3347731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>upos</th>\n",
       "      <th>xpos</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>मैं</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>4</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>खाने</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VM</td>\n",
       "      <td>4</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>नहीं</td>\n",
       "      <td>PART</td>\n",
       "      <td>NEG</td>\n",
       "      <td>4</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>जाऊँगा</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VM</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>।</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>SYM</td>\n",
       "      <td>4</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text   upos xpos  head  deprel\n",
       "0     मैं   PRON  PRP     4   nsubj\n",
       "1    खाने   VERB   VM     4   advcl\n",
       "2    नहीं   PART  NEG     4  advmod\n",
       "3  जाऊँगा   VERB   VM     0    root\n",
       "4       ।  PUNCT  SYM     4   punct"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract relevant fields from the data\n",
    "df_data = [(token['text'], token['upos'], token['xpos'], token['head'], token['deprel']) for token in doc.to_dict()[0]]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df_data, columns=['text', 'upos', 'xpos', 'head', 'deprel'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5687e-97ff-4577-9fd8-fbc625a069ce",
   "metadata": {},
   "source": [
    "# Extracting Tags for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cad33a4b-af56-4971-8c93-f90ea08ace67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags for hindi sentences\n",
    "word_tag = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        word_dict = {\n",
    "            \"text\": word.text,\n",
    "            \"xpos\": word.xpos,\n",
    "            \"id\": word.id,\n",
    "            \"lemma\": word.lemma,\n",
    "            \"deprel\": word.deprel,\n",
    "            \"head\": word.head\n",
    "        }\n",
    "        word_tag.append(word_dict)\n",
    "#print(word_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa79477-752b-4a55-8d60-81ce4728707c",
   "metadata": {},
   "source": [
    "# Removing Unwanted Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "858e6bc7-2626-4339-8e2b-0fe6a2145902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Unwanted Tags - VAUX CC SYM\n",
    "unwanted_tags = ['VAUX','CC','SYM','PSP']\n",
    "word_tag_cleaned = {}\n",
    "for rel_dict in word_tag:\n",
    "    if rel_dict['xpos'] not in unwanted_tags:\n",
    "        word_tag_cleaned[rel_dict['id']] =  rel_dict\n",
    "#print(word_tag_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ee777d6-4bcf-42a6-a562-40e15e0613ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मैं', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nsubj', 'head': 4}\n",
      "2 {'text': 'खाने', 'xpos': 'VM', 'id': 2, 'lemma': 'खा', 'deprel': 'advcl', 'head': 4}\n",
      "3 {'text': 'नहीं', 'xpos': 'NEG', 'id': 3, 'lemma': 'नहीं', 'deprel': 'advmod', 'head': 4}\n",
      "4 {'text': 'जाऊँगा', 'xpos': 'VM', 'id': 4, 'lemma': 'जाऊ', 'deprel': 'root', 'head': 0}\n"
     ]
    }
   ],
   "source": [
    "for key,value in word_tag_cleaned.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659148b0-9d55-4978-9d21-f2f8fd04b7d3",
   "metadata": {},
   "source": [
    "# Grammer Rules based Reordering\n",
    "\n",
    "1. Noun Verb\n",
    "\n",
    "2. Assumption:  Subject always comes before object\n",
    "    राम ने श्याम को कहा\n",
    "    Subject Object Verb\n",
    "\n",
    "3. Verb Auxiliary Verb\n",
    "\n",
    "4. Verb Adverb\n",
    "\n",
    "5. Noun Adjective\n",
    "\n",
    "-----------------------------------------------------\n",
    "#### Subjective S_Adjective Object O_Adjective Verb Adverb\n",
    "-----------------------------------------------------\n",
    "#### Arrange in Order : Pronoun Subjective S_Adjective Object O_Adjective -> Noun Adjective Verb Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f240f2c0-ff10-4bbc-a62b-fe1bd430beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dict_order(sample_dict,row_1,row_2):\n",
    "    if row_1 == row_2:\n",
    "        return sample_dict\n",
    "    list_form = [(key,value) for key,value in sample_dict.items()]\n",
    "    index_1 = None     # index_1 \n",
    "    index_2 = None\n",
    "    for i,content in enumerate(list_form):\n",
    "        if content[0] == row_1:\n",
    "            index_1 = i\n",
    "        elif content[0] == row_2:\n",
    "            index_2 = i\n",
    "    entry_1 = list_form.pop(index_1)\n",
    "    list_form.insert(index_2,entry_1)    \n",
    "    return_dict = {}\n",
    "    for key,value in list_form:\n",
    "        return_dict[key] = value\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7e690-9dbb-4bf2-820b-71714d96b959",
   "metadata": {},
   "source": [
    "# Format Subject and Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "631b7b33-bb92-468d-b1b9-e054fc1a3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sign_form = word_tag_cleaned.copy()\n",
    "# Format Subject then Object\n",
    "subject_id = None\n",
    "object_id = None\n",
    "subject_index = None \n",
    "object_index = None\n",
    "cnt = 0\n",
    "for key,value in word_tag_cleaned.items():\n",
    "    if value['deprel'] in ['obj','obl']:\n",
    "        object_id = key\n",
    "        object_index = cnt\n",
    "    elif value['deprel']=='nsubj':\n",
    "        subject_id = key\n",
    "        subject_index = cnt\n",
    "    cnt+=1\n",
    "#print(subject_id,object_id)\n",
    "if subject_id!=None and object_id!=None and subject_id>object_id:\n",
    "    word_sign_form = change_dict_order(word_sign_form,object_id,subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ee030097-493a-47f8-81e7-a09343aa597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मैं', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nsubj', 'head': 4}\n",
      "2 {'text': 'खाने', 'xpos': 'VM', 'id': 2, 'lemma': 'खा', 'deprel': 'advcl', 'head': 4}\n",
      "3 {'text': 'नहीं', 'xpos': 'NEG', 'id': 3, 'lemma': 'नहीं', 'deprel': 'advmod', 'head': 4}\n",
      "4 {'text': 'जाऊँगा', 'xpos': 'VM', 'id': 4, 'lemma': 'जाऊ', 'deprel': 'root', 'head': 0}\n"
     ]
    }
   ],
   "source": [
    "for key,value in word_sign_form.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8027d0-6fea-4f45-8027-e1132acfc13c",
   "metadata": {},
   "source": [
    "# Handling Adjective and Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "12b3bc49-fd8e-4d95-a55c-2730263f557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Adjective and Adverb\n",
    "for key,value in word_tag_cleaned.items():    # word_tag_cleaned\n",
    "    if value['xpos'] in ['JJ']:     # Adjective Adverb\n",
    "        # first condition is for when it does not have corresponding noun or verb - मैं खुश हूं।\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['NN']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "    elif value['xpos'] in ['RB']:\n",
    "        if value['head']!=0 and word_tag_cleaned[value['head']]['xpos'] in ['VM','VAUX']:\n",
    "            word_sign_form = change_dict_order(word_sign_form,key,value['head'])\n",
    "#print(word_sign_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d039b79-0f1d-4c9e-b979-ecfcd42fd957",
   "metadata": {},
   "source": [
    "# Handling Negative Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "86d0f059-dd69-49d8-9f1a-01a450617fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange Negative Sentences\n",
    "for key,value in word_sign_form.items():\n",
    "    if value['xpos']=='NEG': \n",
    "        last_key = list(word_sign_form.keys())[-1]\n",
    "        word_sign_form = change_dict_order(word_sign_form,key,last_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd38bf-0d02-4dc4-9efa-b71868de7d83",
   "metadata": {},
   "source": [
    "# Handling Interrogative Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b6dce53b-e21c-41b1-84db-bb56a011bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Interrogative Sentence\n",
    "for key,value in word_sign_form.items():\n",
    "    if value['xpos']=='WQ': \n",
    "        last_key = list(word_sign_form.keys())[-1]\n",
    "        word_sign_form = change_dict_order(word_sign_form,key,last_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec4a17-c6ad-4d7b-8197-c207e004e128",
   "metadata": {},
   "source": [
    "# StopWord Removal - For Sentences like मैं खुश हूं।\n",
    "#### हूं - comes out to be VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6d6aa9b6-a5f5-40be-b999-51efa9cf8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords from file\n",
    "with open('./utility/final_stopwords.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    stopword_list = file.readlines()\n",
    "stopword_list = [word.strip() for word in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a5a534df-a946-479a-be91-951539f7b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWord Removal\n",
    "stopword_removed_list = {}\n",
    "for key,value in word_sign_form.items():\n",
    "    #print(value['text'] in stopword_list)\n",
    "    if value['text'] not in stopword_list:\n",
    "        stopword_removed_list[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bd1c67d0-8a61-4bdf-a55f-220fbaa30476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'text': 'मैं', 'xpos': 'PRP', 'id': 1, 'lemma': 'मैं', 'deprel': 'nsubj', 'head': 4}\n",
      "2 {'text': 'खाने', 'xpos': 'VM', 'id': 2, 'lemma': 'खा', 'deprel': 'advcl', 'head': 4}\n",
      "4 {'text': 'जाऊँगा', 'xpos': 'VM', 'id': 4, 'lemma': 'जाऊ', 'deprel': 'root', 'head': 0}\n",
      "3 {'text': 'नहीं', 'xpos': 'NEG', 'id': 3, 'lemma': 'नहीं', 'deprel': 'advmod', 'head': 4}\n"
     ]
    }
   ],
   "source": [
    "for key,value in stopword_removed_list.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ca02-4b16-4d06-b580-3bdc2297492a",
   "metadata": {},
   "source": [
    "# Mapping xpos to POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0c99404b-f6b5-4efb-9a1c-3ccd3424e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping xpos to POS tags\n",
    "xpos_to_pos = {\n",
    "    'NNP': 'pnoun',\n",
    "    'VM': 'verb',\n",
    "    'VAUX': 'verb',\n",
    "    'JJ': 'adjective',\n",
    "    'RB': 'adverb',\n",
    "    'PRP' : 'pronoun',\n",
    "    'NEG' : 'negative',\n",
    "    'NN' : 'noun',\n",
    "    'RDP' : 'adverb',\n",
    "    'QF': 'adjective',            # 'अधिक'\\\n",
    "    'WQ': 'wh_adverb',\n",
    "    'NST': 'noun',\n",
    "    'DEM': 'noun_refer_specific',\n",
    "    'INTF': 'intensifier',\n",
    "    # Add more mappings as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9d4298ba-cb9d-4d2c-95ea-730d5a399596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('मैं', 'pronoun'), ('खाने', 'verb'), ('जाऊँगा', 'verb'), ('नहीं', 'negative')]\n"
     ]
    }
   ],
   "source": [
    "# Extract Words from Parser and corresponding tag\n",
    "sign_words_list = []\n",
    "for key,value in stopword_removed_list.items():\n",
    "    if value['xpos'] in xpos_to_pos:\n",
    "        sign_words_list.append((value['text'], xpos_to_pos[value['xpos']]))\n",
    "    else:\n",
    "        sign_words_list.append((value['text'], 'extra'))\n",
    "print(sign_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b50dac-bc18-44eb-98d4-7aed38eba4f5",
   "metadata": {},
   "source": [
    "# READ ISL Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0236114d-93e8-4a6b-8705-9619b997c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ISL Dictionary\n",
    "# Open the file in read mode\n",
    "with open('./utility/isl_dict.txt', 'r',encoding='utf8') as file:\n",
    "    # Read the entire contents of the file\n",
    "    isl_dict = ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fbf1d943-8d0d-43ad-ad8f-1f4587869c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Case : Why_(Sign_2) should be why\n",
    "# Remove content within parentheses and strip whitespace for keys containing \"_(*)\"\n",
    "isl_dict = {re.sub(r'_\\(.*\\)', '', key).strip().lower(): value for key, value in isl_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a2a794e3-b0b0-444c-87cd-d3f9cf4ef99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with lowercase keys\n",
    "isl_dict = {key.lower(): value for key, value in isl_dict.items()}\n",
    "isl_dict['school'] = 'स्कूल'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6ea41ba-a207-4733-81ca-e1500eaa2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Translator\n",
    "from googletrans import Translator\n",
    "# Create a Translator object\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5558836-b586-4fc4-9daf-c7b368cbde42",
   "metadata": {},
   "source": [
    "# Synonym Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1956169d-5b79-4e73-a3fa-48e1d9f24ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25:12:02:51,676 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_load_synset', '_load_synset_file', '_load_synset_relations', '_relation_list', '_synset_df', '_synset_idx_map', '_synset_relations_dict', '_update_synset_idx_map', 'all_synsets', 'all_words', 'synset_relation', 'synsets']\n",
      "[Synset('आम.noun.3462')]\n",
      "<bound method IndoWordNet.all_synsets of <pyiwn.iwn.IndoWordNet object at 0x0000016994424550>>\n"
     ]
    }
   ],
   "source": [
    "# Synonym Substitution\n",
    "import pyiwn\n",
    "from nltk.corpus import wordnet as wn\n",
    "iwn = pyiwn.IndoWordNet() \n",
    "print(dir(iwn))\n",
    "print(iwn.synsets('आम्र'))\n",
    "print(iwn.all_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "76cfa64c-898b-4d10-be2f-8001c0fe10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Special Case for Mapping to Videos\n",
    "special_videos = {\n",
    "    'i' : '@D:\\\\desktop\\\\project\\\\Linguistic\\\\I\\\\I_Me.mp4',\n",
    "    'who': '@D:\\\\desktop\\\\project\\\\Linguistic\\\\W\\\\Who_Whom.mp4',\n",
    "    'whom': '@D:\\\\desktop\\\\project\\\\Linguistic\\\\W\\\\Who_Whom.mp4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "62945f12-845e-4762-b1b1-6bfd8a9b698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मैं i\n",
      "खाने eating\n",
      "जाऊँगा will go\n",
      "नहीं no\n",
      "[('मैं', 'pronoun', '@D:\\\\desktop\\\\project\\\\Linguistic\\\\I\\\\I_Me.mp4'), ('खाने', 'verb', 'eat'), ('जाऊँगा', 'verb', '#'), ('नहीं', 'negative', 'no')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "synonym_substituted_list = []\n",
    "temp_list = [('आकलन', 'noun')]\n",
    "for word,pos_tag in sign_words_list:\n",
    "\n",
    "    # Translate the Hindi sentence to English\n",
    "    english_word = translator.translate(word, src='hi', dest='en').text.lower()\n",
    "    english_word_lemmatized = lemmatizer(english_word)[0].lemma_.lower()\n",
    "    print(word,english_word)\n",
    "    \n",
    "    if english_word_lemmatized in list(special_videos.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,special_videos[english_word_lemmatized]))\n",
    "        continue\n",
    "    \n",
    "    if pos_tag == 'pnoun':\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    #print(word,pos_tag)\n",
    "    \n",
    "    # Case 1 : Check hindi word in isl_dict\n",
    "    if word in list(isl_dict.values()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "    all_hindi_synsets = []\n",
    "    # Case 2 : Check synonym of hindi_word in isl_dict\n",
    "    try:\n",
    "        all_hindi_synsets = iwn.synsets(word)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    flag = False\n",
    "    for synset in all_hindi_synsets:\n",
    "        if synset._head_word in list(isl_dict.values()):\n",
    "            corresponding_keys = [key for key, value in isl_dict.items() if value == synset._head_word]\n",
    "            synonym_substituted_list.append((word,pos_tag,corresponding_keys[0]))\n",
    "            flag = True\n",
    "            break\n",
    "    if flag == True:\n",
    "        continue\n",
    "                \n",
    "    # Case 3 : Check english word in isl_dict\n",
    "    if english_word in list(isl_dict.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word))\n",
    "        continue\n",
    "\n",
    "    # Case 4 : Check lemmatized english word in isl_dict\n",
    "    if english_word_lemmatized in list(isl_dict.keys()):\n",
    "        synonym_substituted_list.append((word,pos_tag,english_word_lemmatized))\n",
    "        continue\n",
    "\n",
    "    # Case 5 : Check syno of english word in isl_dict\n",
    "    all_english_synsets = wn.synonyms(english_word)\n",
    "    #print(all_english_synsets)\n",
    "    all_english_synsets_flatten = []\n",
    "    for row in all_english_synsets:\n",
    "        all_english_synsets_flatten.extend(row)\n",
    "    flag = False\n",
    "    for synset in all_english_synsets_flatten:\n",
    "        if synset.lower() in list(isl_dict.keys()):\n",
    "            flag = True\n",
    "            # print('Yes Present')\n",
    "            synonym_substituted_list.append((word,pos_tag,synset.lower()))\n",
    "            break\n",
    "    if flag == True:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Case 6  : Nothing Words Go for Finger Spelling\n",
    "    synonym_substituted_list.append((word,pos_tag,'#'))\n",
    "print(synonym_substituted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2a4a7964-5c69-4e45-b9c0-56b502243502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>ISL Dictionary Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>मैं</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>@D:\\desktop\\project\\Linguistic\\I\\I_Me.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>खाने</td>\n",
       "      <td>verb</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>जाऊँगा</td>\n",
       "      <td>verb</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>नहीं</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hindi Word   POS Tag                         ISL Dictionary Tag\n",
       "0        मैं   pronoun  @D:\\desktop\\project\\Linguistic\\I\\I_Me.mp4\n",
       "1       खाने      verb                                        eat\n",
       "2     जाऊँगा      verb                                          #\n",
       "3       नहीं  negative                                         no"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final ISL List\n",
    "final_isl_list = synonym_substituted_list.copy()\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(final_isl_list, columns=['Hindi Word', 'POS Tag', 'ISL Dictionary Tag'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1570af-2dcc-4bd0-a476-4f2400368ac5",
   "metadata": {},
   "source": [
    "# Video Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f32f543b-b8b2-4e3f-8e83-230f9aea59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversed dictionary mapping Hindi words to English words\n",
    "isl_hindi_english_dict = {hindi_word: english_word for english_word, hindi_word in isl_dict.items()}\n",
    "#print(isl_hindi_english_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4dcc232e-7ded-4ef5-874e-d889430f16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of Devanagari vowel signs to their vowels\n",
    "sign_mapping_vowels = {\n",
    "    'ा': 'आ',  # Aa\n",
    "    'ि': 'इ',  # I\n",
    "    'ी': 'ई',  # II\n",
    "    'ु': 'उ',  # U\n",
    "    'ू': 'ऊ',  # UU\n",
    "    'ृ': 'ऋ',  # R\n",
    "    'े': 'ए',  # E\n",
    "    'ै': 'ऐ',  # AI\n",
    "    'ो': 'ओ',  # O\n",
    "    'ौ': 'औ',  # AU\n",
    "    'ं': 'अं', # Anusvara\n",
    "    'ः': 'अः'  # Visarga\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "16cca5fc-cb58-4d8a-9971-6aa20cb54e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def search_videos(folder_path, final_isl_list):\n",
    "    \"\"\"\n",
    "    Searches for video files named after the provided words in a directory.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the directory containing video files.\n",
    "        words: A list of words to search for (video names).\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to the found video files.\n",
    "    \"\"\"\n",
    "\n",
    "    found_videos = []\n",
    "    # Assuming `words` is a list of tuples like [(or_word1, pos_tag1, isl_word1), (or_word2, pos_tag2, isl_word2), ...]\n",
    "    for or_word, pos_tag, isl_word in final_isl_list:\n",
    "        video_name = f\"{isl_word.capitalize()}.mp4\"\n",
    "        if pos_tag == 'pnoun':  # Alphabets and FingerSpell\n",
    "            for letter in isl_word:\n",
    "                video_name = f\"{letter.capitalize()}.mp4\"\n",
    "                for root, dirs, files in os.walk(folder_path):\n",
    "                    full_path = os.path.join(root, video_name)\n",
    "                    if os.path.isfile(full_path):\n",
    "                        found_videos.append(full_path)\n",
    "        elif isl_word=='#':\n",
    "            for char in or_word:\n",
    "                if char in sign_mapping_vowels.keys():\n",
    "                    video_name = f\"{sign_mapping_vowels[char]}.mp4\"\n",
    "                else:\n",
    "                    video_name = f\"{char}.mp4\"\n",
    "                for root, dirs, files in os.walk(folder_path):\n",
    "                    full_path = os.path.join(root, video_name)\n",
    "                    if os.path.isfile(full_path):\n",
    "                        found_videos.append(full_path)\n",
    "        elif isl_word[0]=='@':         # Special Words are handled here\n",
    "            found_videos.append(isl_word[1:])\n",
    "        else:\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                full_path = os.path.join(root, video_name)\n",
    "                if os.path.isfile(full_path):\n",
    "                    found_videos.append(full_path)\n",
    "\n",
    "    return found_videos\n",
    "\n",
    "\n",
    "def play_videos(video_paths):\n",
    "    vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe'  # Path to VLC media player executable\n",
    "    for video_path in video_paths:\n",
    "        subprocess.Popen([vlc_path, '--fullscreen', video_path])\n",
    "        time.sleep(5)  # Delay before playing the next video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79086730-2fda-46d2-bb7a-d10c9357455e",
   "metadata": {},
   "source": [
    "# Set Folder Path where ISL videos are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f34f5062-592d-4673-8931-c9b9a8a84383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it accordingly\n",
    "folder_path = 'D:\\desktop\\project\\Linguistic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "72107c58-36dd-4bf2-b396-1b2e4c5b7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = search_videos(folder_path, final_isl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "072f48c5-87c7-46e7-954c-07a78e698a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video merged_video.mp4.\n",
      "MoviePy - Writing audio in merged_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video merged_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  51%|██████████████████████████████████▎                                | 451/880 [00:13<00:13, 31.59it/s, now=None]2024-04-25:12:03:21,991 WARNING  [warnings.py:109] C:\\Users\\Lenovo\\Desktop\\Project\\Linguistic Project\\env\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file D:\\desktop\\project\\Linguistic\\hindi_alphabets\\ISL\\आ.mp4, 6220800 bytes wanted but 0 bytes read,at frame 93/94, at time 3.10/3.12 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n",
      "t:  79%|████████████████████████████████████████████████████▊              | 694/880 [00:21<00:05, 31.25it/s, now=None]2024-04-25:12:03:30,466 WARNING  [warnings.py:109] C:\\Users\\Lenovo\\Desktop\\Project\\Linguistic Project\\env\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file D:\\desktop\\project\\Linguistic\\hindi_alphabets\\ISL\\ग.mp4, 6220800 bytes wanted but 0 bytes read,at frame 142/143, at time 4.73/4.76 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready merged_video.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['C:\\\\Program Files\\\\VideoLAN\\\\VLC\\\\vlc.exe',...>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "vlc_path = r'C:\\Program Files\\VideoLAN\\VLC\\vlc.exe' \n",
    "def merge_videos(video_paths):\n",
    "    clips = [VideoFileClip(path) for path in video_paths]\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    return final_clip\n",
    "\n",
    "# Merge the videos into a single video\n",
    "merged_clip = merge_videos(video_paths)\n",
    "\n",
    "\n",
    "# Export the merged video to a file\n",
    "merged_clip.write_videofile(\"merged_video.mp4\")\n",
    "\n",
    "# Play the merged video\n",
    "subprocess.Popen([vlc_path, '--fullscreen', 'merged_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad233a8-4ba9-485f-b44b-f4601cfdda21",
   "metadata": {},
   "source": [
    "# Hope You Like the project. :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
